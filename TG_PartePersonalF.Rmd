---
title: "Trabajo grupal"
output: html_document
date: '2022-06-17'
---

```{r}
library(rio)
data=import("Data_World_Bank_2016.xlsx")
```

VARIABLES:

-Dependiente:
    (IVA) INDUSTRIALIZACIÓN, VALOR AGREGADO

- Independientes:
    (GEI) EMISIONES DE GASES DE EFECTO INVERNADERO TOTALES
    (CDA) CONTAMINACIÓN DEL AIRE POR PM2,5


REGRESIÓN #1


```{r}
class(data$GEI)
class(data$IVA)
```

```{r}
data$GEI=as.numeric(data$GEI)
data$IVA=as.numeric(data$IVA)
```

```{r}
class(data$GEI)
class(data$IVA)
```

```{r}
sum(is.na(data$GEI))
sum(is.na(data$IVA))
```

```{r}
data1 = data[complete.cases(data$GEI),]
data1 = data[complete.cases(data$IVA),]
```

```{r}
plot(data1$GEI,data1$IVA)
```

```{r}
library(nortest)
lillie.test(data1$GEI)
```

```{r}
lillie.test(data1$IVA)
```

Ambos p-values son menores a 0.05 por lo cual se procede a continuar las pruebas de correlación con Spearman.



HIPOTESIS:

H0 = No existe correlación entre la industrialización, valor agregado y las emisiones de gases de efecto invernadero totales
H1 = Sí existe correlación entre la industrialización, valor agregado y las emisiones de gases de efecto invernadero totales

```{r}
cor.test(data1$GEI, data1$IVA, method = c("spearman"))
```

Hecho el cor.test, nos arroja un valor de 0.4649 para el p-value por lo que se determina que no hay correlación entre las variables.


Las hipótesis para la prueba F son las siguientes:

H0: El modelo de regresión no es válido

H1: El modelo de regresión es válido


```{r}
modelo1 <- lm(GEI~IVA, data=data)
anova(modelo1)
summary(modelo1)
```

Siendo el valor del p-value 0.6883, se acepta la hipotesis nula y se concluye que GEI no explica IVA.



REGRESIÓN #2

```{r}
class(data$CDA)
class(data$IVA)
```

```{r}
data$CDA=as.numeric(data$CDA)
data$IVA=as.numeric(data$IVA)
```

```{r}
class(data$CDA)
class(data$IVA)
```

```{r}
sum(is.na(data$CDA))
sum(is.na(data$IVA))
```

```{r}
data2 = data[complete.cases(data$CDA),]
data2 = data[complete.cases(data$IVA),]
```

```{r}
plot(data2$CDA,data1$IVA)
```

```{r}
library(nortest)
lillie.test(data2$CDA)
```

```{r}
lillie.test(data2$IVA)
```

Ambos valores p-values son menores a 0.05, dado ello se procede a realizar pruebas de correlación con Spearman. 


HIPOTESIS:
H0 = No existe correlación entre la industrialización, valor agregado y la contaminación del aire por PM2,5 
H1 = Sí existe correlación entre la industrialización, valor agregado y la contaminación del aire por PM2,5 

```{r}
cor.test(data2$CDA, data2$IVA, method = c("spearman"))
```

En este caso, el p-value es mayor a 0.05 por lo que se aprueba la hipotesis nula y se asume que no hay mayor relación entre variables



Las hipótesis para la prueba F son las siguientes:

H0: El modelo de regresión no es válido

H1: El modelo de regresión es válido


```{r}
modelo2 <- lm(CDA~IVA, data=data)
anova(modelo2)
summary(modelo2)
```

Dado que el p-value es mayor a 0.05, se aprueba la hipotesis nula, siendo asi el modelo invalido y se concluye que no existe mayor relacion entre CDA y IVA.


ECUACIÓN: Y= 2.2329+3.2187*X

```{r}
library(ggplot2)
ggplot(data2, aes(x=GEI, y=IVA)) +
  geom_point(colour="brown") +  xlab("Emisiones de gases de efecto invernadero totales") +  ylab("Industrialización, valor agregado") +
  ggtitle("Modelo 2") +
  theme_light()+ geom_smooth(method="lm", se = F)
```

Regresión lineal multivariada:

```{r}
library(stargazer)
```

```{r}
modelo3=formula(data$IVA~data$GEI+data$CDA)
regresión1=lm(modelo3,data=data)
stargazer(regresión1,type = "text")
```

```{r}
summary(regresión1)
```

Este modelo no aporta dado que el p-value es mayor a 0.05, de igual manera, los p-value de GEI y CDA son mayores a 0.05.

Diagnósticos de regresión:

- Linealidad:

```{r}
plot(regresión1, 1)
```

- Homocedasticidad:

```{r}
plot(regresión1, 3)
```

```{r}
library(lmtest)
```

```{r}
bptest(regresión1)
```

La probabilidad de homocedasticidad es muy alta porque el p-value tiene un valor mayor a 0.05, se aprueba que el modelo tenga homocedasticidad.

- Normalidad de residuos:

```{r}
plot(regresión1, 2)
```

```{r}
shapiro.test(regresión1$residuals)
```

```{r}
install.packages("DescTools")
library(DescTools)
VIF(regresión1)
```

Como ninguna variable sale mayor a 5, ninguna es candidata a ser retirada y muestra que estas no están muy correlacionadas entre sí, es decir, muestra que estas no tratan de explicar el mismo fenómeno

- Valores influyentes:

```{r}
plot(regresión1, 5)
```

El gráfico muestra que efectivamente hay valores atípicos que influyen negativamente en la regresión


Análisis de conglomerados:

```{r}
list(names(data))
```

```{r}
data3 = data[,c(1,8,12,13)]
```

```{r}
install.packages("readr")
library(readr)
str(data3)
```

```{r}
summary(data3)
```

```{r}
data3$UET=as.numeric(data3$UET)
```

```{r}
class(data3$UET)
```
```{r}
sum(is.na(data3$UET))
```

```{r}
boxplot(data3[,-1])
```

```{r}
install.packages("BBmisc")
library(BBmisc)
```

```{r}
boxplot(normalize(data3[,-1],method='range',range=c(0,1)))
```

```{r}
data3[,-1]=normalize(data3[,-1],method='standardize')
data3=data3[complete.cases(data3),]

summary(data3)
```

- Correlaciones:

```{r}
cor(data3[,-1])
```

Cambio de monotonía:

```{r}
data3$IVA=-1*data3$IVA

cor(data3[,-1])
```

Preparamos la data para la clusterización:

```{r}
dataClus=data3[,-1]
row.names(dataClus)=data3$`Country Name`
```

CLAUSTERIZACIÓN:

```{r}
install.packages("cluster")
library(cluster)
g.dist = daisy(dataClus, metric="gower")
```

Pondremos cuatro clusters:

```{r}
set.seed(123)
pam.resultado=pam(g.dist,4,cluster.only = F)

dataClus$pam=pam.resultado$cluster
```

Exploración de resultados:

```{r}
aggregate(.~ pam, data=dataClus,mean)
```

Recodificamos las etiquetas del cluster:

```{r}
original=aggregate(.~ pam, data=dataClus,mean)
original[order(original$IVA),]
```

Se va a recodificar los clusters en función del IVA (Industrialización, valor agregado), se tomo está decisión porque es la variable dependiente y la que más nos interesa analizar

```{r}
dataClus$pam=dplyr::recode(dataClus$pam, `3` = 1, `2`=2,`4`=3,`1`=4)
```

ESTRATEGIA JERARQUICA:

Estrategia jerarquica aglomerativa:

```{r}
set.seed(123)
install.packages("factoextra")
library(factoextra)
```

```{r}
res.agnes<- hcut(g.dist, k = 4,hc_func='agnes',hc_method = "ward.D")

dataClus$agnes=res.agnes$cluster
```

```{r}
aggregate(.~ agnes, data=dataClus,mean)
```

Recodificamos el cluster para que este ordenado:

```{r}
original=aggregate(.~ agnes, data=dataClus,mean)
original[order(original$IVA),]
```

```{r}
dataClus$agnes=dplyr::recode(dataClus$agnes, `4` = 1, `1`=2,`3`=3,`2`=4)
```

Visualización:

```{r}
fviz_dend(res.agnes, cex = 0.7, horiz = T)
```

Estrategia jerárquica divisiva:

```{r}
proyeccion = cmdscale(g.dist, k=2,add = T)
dataClus$dim1 <- proyeccion$points[,1]
dataClus$dim2 <- proyeccion$points[,2]
```

```{r}
base= ggplot(dataClus,aes(x=dim1, y=dim2,label=row.names(dataClus))) 
base + geom_text(size=2)
```

```{r}
base= ggplot(dataClus,aes(x=dim1, y=dim2)) +  coord_fixed()
base + geom_point(size=2, aes(color=as.factor(pam)))  + labs(title = "PAM") 
```

```{r}
base= ggplot(dataClus,aes(x=dim1, y=dim2)) +  coord_fixed()
base + geom_point(size=2, aes(color=as.factor(agnes))) + labs(title = "AGNES")
```

```{r}
base= ggplot(dataClus,aes(x=dim1, y=dim2)) +  coord_fixed()
base + geom_point(size=2, aes(color=as.factor(diana))) + labs(title = "DIANA")
```

```{r}
g.dist.cmd = daisy(dataClus[,c('dim1','dim2')], metric = 'euclidean')
```

```{r}
install.packages("dbscan")
library(dbscan)
kNNdistplot(g.dist, k=3)
```

```{r}
install.packages("fpc")
library(fpc)
```

```{r}
db.cmd = fpc::dbscan(g.dist, eps=0.065, MinPts=3,method = 'dist')
```

```{r}
db.cmd
```

```{r}
data3$db=as.factor(db.cmd$cluster)
```


```{r}
library(ggrepel)
base= ggplot(dataClus[dataClus$db!=0,],aes(x=dim1, y=dim2)) + coord_fixed()

dbplot= base + geom_point(aes(color=db)) 

dbplot + geom_point(data=dataClus[dataClus$db.cmd==0,],
                    shape=0) 
```

##. Analisis factorial (tus propias variables + otras del grupo): solo grafica de factores

###Proceso del Analisis Factorial Exploratorio (EFA)

1.Matriz de correlación:

```{r}
dontselect=c("Country")
select=setdiff(names(dataClus),dontselect) 
Data=dataClus[,select]
```

```{r}
library(polycor)
corMatrix=polycor::hetcor(Data)$correlations
```

2, Explorar correlaciones:

```{r}
install.packages("ggcorrplot")
library(ggcorrplot)
ggcorrplot(corMatrix)
```

```{r}
install.packages("psych")
library(psych)
```

```{r}
psych::KMO(corMatrix) 
```

```{r}
cortest.bartlett(corMatrix,n=nrow(Data))$p.value>0.05
```

